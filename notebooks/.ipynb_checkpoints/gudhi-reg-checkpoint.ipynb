{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f39ca12f9f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gudhi as gd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import networkx as nx\n",
    "import scipy.sparse\n",
    "from nn_homology import nn_graph\n",
    "\n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "card  = 100    # max number of points in the diagrams\n",
    "hom   = 0     # homological dimension\n",
    "ml    = 10.   # max distance in Rips\n",
    "lr    = 1e-1  # learning rate\n",
    "lbda  = 0.95  # hyperparameter for topological loss\n",
    "percentile = 90 # threshold percentile on weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_persistence(st, card, hom_dim):\n",
    "    st.persistence()\n",
    "    pairs = st.persistence_pairs()\n",
    "    dgm = np.zeros([card, 2], dtype=np.float32)\n",
    "    spl = np.zeros([card, 2*hom_dim + 3], dtype=np.int32)\n",
    "    mask = np.zeros([card], dtype=np.int32)\n",
    "    count = 0\n",
    "    for [splx0, splx1] in pairs:\n",
    "        # splx0 is negative simplex, splx1 is positive simplex\n",
    "        # these are arrays of ??indices?? that will be of length hom_dim+1\n",
    "        if len(splx0) - 1 == hom_dim and count < card and len(splx1) > 0:\n",
    "            dgm[count,0], dgm[count,1] = st.filtration(splx0), st.filtration(splx1)\n",
    "            # store the positive and negative simplices\n",
    "            spl[count,:hom_dim+1], spl[count,hom_dim+1:] = np.array(splx0), np.array(splx1)\n",
    "            # and track which points are relevant with mask\n",
    "            mask[count] = 1\n",
    "            count += 1\n",
    "    return [dgm, spl, mask]\n",
    "\n",
    "def compute_rips(card, hom_dim, D, max_length):\n",
    "    rc = gd.RipsComplex(distance_matrix=D)\n",
    "    st = rc.create_simplex_tree(max_dimension=hom_dim+1)\n",
    "    return compute_persistence(st, card, hom_dim)\n",
    "\n",
    "def compute_rips_grad(grad_dgm, dgm, spl, mask, c, h, params, adj, idx_vec, tol=1e-6):\n",
    "    grad_x = torch.zeros(params.shape)\n",
    "    for i in range(c[0]):\n",
    "        if mask[i] == 1:\n",
    "            val0, val1 = dgm[i,0], dgm[i,1]\n",
    "            splx0, splx1 = spl[i,:h[0]+1], spl[i,h[0]+1:]\n",
    "            # get rows in distance matrix according to each face of each simplex\n",
    "            D0, D1 = adj[splx0,:][:, splx0], adj[splx1,:][:, splx1]\n",
    "            \n",
    "            # find maximally distant simplices in filtration\n",
    "            [v0a, v0b] = list(splx0[np.argwhere(np.abs(D0-val0) <= tol)[0,:]])\n",
    "            [v1a, v1b] = list(splx1[np.argwhere(np.abs(D1-val1) <= tol)[0,:]])\n",
    "            \n",
    "            v0a = idx_vec[v0a]\n",
    "            v0b = idx_vec[v0b]\n",
    "            \n",
    "            v1a = idx_vec[v1a]\n",
    "            v1b = idx_vec[v1b]\n",
    "            \n",
    "            # v0a and v0b refer to the indices of the simplices in the lower dimension\n",
    "            # v1a and v1b refer to the indices of the simplices in the higher dimension\n",
    "            if h[0] > 0:\n",
    "                grad_x[v0a] += grad_dgm[i,0] * (params[v0a] - params[v0b]) / val0\n",
    "                grad_x[v0b] += grad_dgm[i,0] * (params[v0b] - params[v0a]) / val0\n",
    "            grad_x[v1a] += grad_dgm[i,1] * (params[v1a] - params[v1b]) / val1\n",
    "            grad_x[v1b] += grad_dgm[i,1] * (params[v1b] - params[v1a]) / val1\n",
    "    return grad_x\n",
    "\n",
    "class Rips(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, params, adj, idx_vec, card, hom_dim, max_length):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        \n",
    "        dgm, spl, mask = compute_rips(card, hom_dim, adj, max_length)\n",
    "        ctx.dgm = dgm\n",
    "        ctx.spl = spl\n",
    "        ctx.card = card\n",
    "        ctx.hom_dim = hom_dim\n",
    "        ctx.adj = adj\n",
    "        ctx.idx_vec = idx_vec\n",
    "        ctx.params = params\n",
    "        return torch.tensor(dgm), torch.tensor(spl), torch.tensor(mask)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dgm_grad, spl_grad, mask_grad):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        dgm = ctx.dgm\n",
    "        spl = ctx.spl\n",
    "        c = ctx.card\n",
    "        h = ctx.hom_dim\n",
    "        params = ctx.params\n",
    "        adj = ctx.adj\n",
    "        idx_vec = ctx.idx_vec\n",
    "        grad_x = compute_rips_grad(dgm_grad.detach().numpy(), dgm, spl, mask, c, h, params, adj, idx_vec)\n",
    "        return None, None, grad_x, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 3)\n",
    "        self.fc1 = nn.Linear(6912, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        self.param_info = [{'layer_type': 'Conv2d', 'kernel_size':(3,3), 'stride':1, 'padding': 0, 'name':'Conv1'},\n",
    "                            {'layer_type': 'Conv2d', 'kernel_size':(3,3), 'stride':1, 'padding':0, 'name':'Conv2'},\n",
    "                            {'layer_type':'Linear', 'name': 'Linear1'},\n",
    "                            {'layer_type':'Linear', 'name': 'Linear2'}]\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "def sum_diag_loss(output, dgm, target, lbda=0.3):\n",
    "    diagloss = -torch.sum((dgm[:,1]-dgm[:,0])**2)\n",
    "    fn = F.nll_loss(output,target)\n",
    "    print(diagloss.detach().cpu().numpy(), fn.detach().cpu().numpy())\n",
    "    return (1-lbda)*fn + lbda*diagloss\n",
    "\n",
    "def flatten_params_torch(param_info, device):\n",
    "    param_vecs = []\n",
    "    for param in param_info:\n",
    "        if param['layer_type'] == 'Conv2d':\n",
    "            p = param['param']\n",
    "            param_vecs.append(p.reshape(p.shape[0],-1).flatten())\n",
    "        if param['layer_type'] == 'Linear':\n",
    "            p = param['param']\n",
    "            param_vecs.append(p.flatten())\n",
    "\n",
    "    # make the first element zero (could be anything given we're filtering below)\n",
    "    param_vecs = [torch.zeros(1).to(device)] + param_vecs\n",
    "    param_vec = torch.cat(param_vecs)\n",
    "\n",
    "    return param_vec\n",
    "\n",
    "def train_regular(model, device, train_loader, optimizer, epoch, log_interval=100):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "def train(model, param_info, G, device, train_loader, optimizer, epoch, lbda, p, log_interval=100, update_every=10):\n",
    "    rips = Rips.apply\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        up = nn_graph.append_params(model.param_info, nn_graph.get_weights(model, tensors=True))\n",
    "        params = flatten_params_torch(up, device)\n",
    "        \n",
    "        if batch_idx % update_every == 0:\n",
    "            params = flatten_params_torch(up, device).cpu().numpy()\n",
    "            thresh = np.percentile(1./(1.+np.abs(params)), p)\n",
    "            G.parameter_graph(model, param_info, (1,1,28,28), update_indices=True, threshold=thresh)\n",
    "        else:\n",
    "            G.update_adjacency(model)\n",
    "        print(len(G.G))\n",
    "        print(G.adj_vec.shape)\n",
    "        print(G.get_adjacency().shape)\n",
    "        output = model(data)\n",
    "        dgm, spl, msk = rips(params, G.get_adjacency(), G.graph_idx_vec, card, hom, ml)\n",
    "        dgmnumpy = dgm.detach().cpu().numpy()\n",
    "#         print(dgmnumpy[dgmnumpy > 0].shape)\n",
    "        loss = sum_diag_loss(output, dgm, target, lbda=lbda)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                datasets.FashionMNIST('../../data', train=True, download=False,\n",
    "                transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=32, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST('../../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=32, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: Conv1\n",
      "Layer: Conv2\n",
      "Layer: Linear1\n",
      "Layer: Linear2\n",
      "14770\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (0,) could not be broadcast to indexing result of shape (109069065,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a4842d042867>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodeldev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNNG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodeldev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-2b8e9d003e20>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, param_info, G, device, train_loader, optimizer, epoch, lbda, p, log_interval, update_every)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_adjacency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_adjacency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mdgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrips\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_adjacency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_idx_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gebhart/projects/nn_homology/nn_homology/nn_graph.py\u001b[0m in \u001b[0;36mget_adjacency\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj_vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m         \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj_vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (0,) could not be broadcast to indexing result of shape (109069065,)"
     ]
    }
   ],
   "source": [
    "NNG = nn_graph.NNGraph()\n",
    "model = Net()\n",
    "\n",
    "modeldev = Net().to(device)\n",
    "optimizer = torch.optim.Adam(modeldev.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    train(modeldev, model.param_info, NNG, device, train_loader, optimizer, epoch, lbda, percentile, update_every=100)\n",
    "    test(modeldev, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# modeldev = Net().to(device)\n",
    "# optimizer = torch.optim.Adam(modeldev.parameters(), lr=0.001)\n",
    "\n",
    "# for epoch in range(1, 2):\n",
    "#     train_regular(modeldev, device, train_loader, optimizer, epoch)\n",
    "#     test(modeldev, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
