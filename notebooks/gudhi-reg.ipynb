{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f48b63719f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gudhi as gd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import networkx as nx\n",
    "import scipy.sparse\n",
    "from nn_homology import nn_graph\n",
    "\n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "card  = 20    # max number of points in the diagrams\n",
    "hom   = 0     # homological dimension\n",
    "ml    = 10.   # max distance in Rips\n",
    "lr    = 1e-1  # learning rate\n",
    "lbda  = 0.95  # hyperparameter for topological loss\n",
    "percentile = 95 # threshold percentile on weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_persistence(st, card, hom_dim):\n",
    "    st.persistence()\n",
    "    pairs = st.persistence_pairs()\n",
    "    dgm = np.zeros([card, 2], dtype=np.float32)\n",
    "    spl = np.zeros([card, 2*hom_dim + 3], dtype=np.int32)\n",
    "    mask = np.zeros([card], dtype=np.int32)\n",
    "    count = 0\n",
    "    for [splx0, splx1] in pairs:\n",
    "        # splx0 is negative simplex, splx1 is positive simplex\n",
    "        # these are arrays of ??indices?? that will be of length hom_dim+1\n",
    "        if len(splx0) - 1 == hom_dim and count < card and len(splx1) > 0:\n",
    "            dgm[count,0], dgm[count,1] = st.filtration(splx0), st.filtration(splx1)\n",
    "            # store the positive and negative simplices\n",
    "            spl[count,:hom_dim+1], spl[count,hom_dim+1:] = np.array(splx0), np.array(splx1)\n",
    "            # and track which points are relevant with mask\n",
    "            mask[count] = 1\n",
    "            count += 1\n",
    "    return [dgm, spl, mask]\n",
    "\n",
    "def compute_rips(card, hom_dim, D, max_length):\n",
    "    D[D == 0] = np.finfo(float).eps # need slightly nonzero values because Gudhi is weird??\n",
    "    rc = gd.RipsComplex(distance_matrix=D)\n",
    "    st = rc.create_simplex_tree(max_dimension=hom_dim+1)\n",
    "    return compute_persistence(st, card, hom_dim)\n",
    "\n",
    "def compute_rips_grad(grad_dgm, dgm, spl, mask, c, h, params, adj, idx_vec, tol=1e-6):\n",
    "    grad_x = torch.zeros(params.shape)\n",
    "    for i in range(c[0]):\n",
    "        if mask[i] == 1:\n",
    "            val0, val1 = dgm[i,0], dgm[i,1]\n",
    "            splx0, splx1 = spl[i,:h[0]+1], spl[i,h[0]+1:]\n",
    "            # get rows in distance matrix according to each face of each simplex\n",
    "            D0, D1 = adj[splx0,:][:, splx0], adj[splx1,:][:, splx1]\n",
    "            \n",
    "            # find maximally distant simplices in filtration\n",
    "            [v0a, v0b] = list(splx0[np.argwhere(np.abs(D0-val0) <= tol)[0,:]])\n",
    "            [v1a, v1b] = list(splx1[np.argwhere(np.abs(D1-val1) <= tol)[0,:]])\n",
    "            \n",
    "            v0a = idx_vec[v0a]\n",
    "            v0b = idx_vec[v0b]\n",
    "            \n",
    "            v1a = idx_vec[v1a]\n",
    "            v1b = idx_vec[v1b]\n",
    "            \n",
    "            # v0a and v0b refer to the indices of the simplices in the lower dimension\n",
    "            # v1a and v1b refer to the indices of the simplices in the higher dimension\n",
    "            if h[0] > 0:\n",
    "                grad_x[v0a] += grad_dgm[i,0] * (params[v0a] - params[v0b]) / val0\n",
    "                grad_x[v0b] += grad_dgm[i,0] * (params[v0b] - params[v0a]) / val0\n",
    "            grad_x[v1a] += grad_dgm[i,1] * (params[v1a] - params[v1b]) / val1\n",
    "            grad_x[v1b] += grad_dgm[i,1] * (params[v1b] - params[v1a]) / val1\n",
    "    return grad_x\n",
    "\n",
    "class Rips(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, params, adj, idx_vec, card, hom_dim, max_length):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        \n",
    "        dgm, spl, mask = compute_rips(card, hom_dim, adj, max_length)\n",
    "        ctx.dgm = dgm\n",
    "        ctx.spl = spl\n",
    "        ctx.card = card\n",
    "        ctx.hom_dim = hom_dim\n",
    "        ctx.adj = adj\n",
    "        ctx.idx_vec = idx_vec\n",
    "        ctx.params = params\n",
    "        return torch.tensor(dgm), torch.tensor(spl), torch.tensor(mask)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dgm_grad, spl_grad, mask_grad):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        dgm = ctx.dgm\n",
    "        spl = ctx.spl\n",
    "        c = ctx.card\n",
    "        h = ctx.hom_dim\n",
    "        params = ctx.params\n",
    "        adj = ctx.adj\n",
    "        idx_vec = ctx.idx_vec\n",
    "        grad_x = compute_rips_grad(dgm_grad.detach().numpy(), dgm, spl, mask, c, h, params, adj, idx_vec)\n",
    "        return None, None, grad_x, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 3)\n",
    "        self.fc1 = nn.Linear(6912, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        self.param_info = [{'layer_type': 'Conv2d', 'kernel_size':(3,3), 'stride':1, 'padding': 0, 'name':'Conv1'},\n",
    "                            {'layer_type': 'Conv2d', 'kernel_size':(3,3), 'stride':1, 'padding':0, 'name':'Conv2'},\n",
    "                            {'layer_type':'Linear', 'name': 'Linear1'},\n",
    "                            {'layer_type':'Linear', 'name': 'Linear2'}]\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "def sum_diag_loss(output, dgm, target, lbda=0.3):\n",
    "    diagloss = -torch.sum((dgm[:,1]-dgm[:,0])**2)\n",
    "    fn = F.nll_loss(output,target)\n",
    "    print(diagloss.detach().cpu().numpy(), fn.detach().cpu().numpy())\n",
    "    return (1-lbda)*fn + lbda*diagloss\n",
    "\n",
    "def flatten_params_torch(param_info, device):\n",
    "    param_vecs = []\n",
    "    for param in param_info:\n",
    "        if param['layer_type'] == 'Conv2d':\n",
    "            p = param['param']\n",
    "            param_vecs.append(p.reshape(p.shape[0],-1).flatten())\n",
    "        if param['layer_type'] == 'Linear':\n",
    "            p = param['param']\n",
    "            param_vecs.append(p.flatten())\n",
    "\n",
    "    # make the first element zero (could be anything given we're filtering below)\n",
    "    param_vecs = [torch.zeros(1).to(device)] + param_vecs\n",
    "    param_vec = torch.cat(param_vecs)\n",
    "\n",
    "    return param_vec\n",
    "\n",
    "def train_regular(model, device, train_loader, optimizer, epoch, log_interval=100):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "def train(model, param_info, G, device, train_loader, optimizer, epoch, lbda, p, log_interval=100, update_every=10):\n",
    "    rips = Rips.apply\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        up = nn_graph.append_params(model.param_info, nn_graph.get_weights(model, tensors=True))\n",
    "        params = flatten_params_torch(up, device)\n",
    "        \n",
    "        if batch_idx % update_every == 0:\n",
    "            params = flatten_params_torch(up, device).cpu().numpy()\n",
    "            thresh = np.percentile(1./(1.+np.abs(params)), p)\n",
    "            G.parameter_graph(model, param_info, (1,1,28,28), update_indices=True, threshold=thresh)\n",
    "        else:\n",
    "            G.update_adjacency(model)\n",
    "        \n",
    "        output = model(data)\n",
    "        dgm, spl, msk = rips(params, G.get_adjacency(), G.graph_idx_vec, card, hom, ml)\n",
    "        dgmnumpy = dgm.detach().cpu().numpy()\n",
    "#         print(dgmnumpy[dgmnumpy > 0].shape)\n",
    "        loss = sum_diag_loss(output, dgm, target, lbda=lbda)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                datasets.FashionMNIST('../../data', train=True, download=False,\n",
    "                transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=32, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST('../../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=32, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: Conv1\n",
      "Layer: Conv2\n",
      "Layer: Linear1\n",
      "Layer: Linear2\n",
      "-9.860761e-31 2.304923\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.115246\n",
      "-9.860761e-31 2.2989836\n",
      "-9.860761e-31 2.2035003\n",
      "-9.860761e-31 2.1120586\n",
      "-9.860761e-31 1.8236247\n",
      "-9.860761e-31 1.7109694\n",
      "-9.860761e-31 1.5530121\n",
      "-9.860761e-31 1.5143003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/schraterlab/anaconda3/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/schraterlab/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/schraterlab/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/schraterlab/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/schraterlab/gebhart/projects/ripsreg/env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-a4842d042867>\", line 8, in <module>\n",
      "    train(modeldev, model.param_info, NNG, device, train_loader, optimizer, epoch, lbda, percentile, update_every=100)\n",
      "  File \"<ipython-input-4-00d1ebd80173>\", line 79, in train\n",
      "    dgm, spl, msk = rips(params, G.get_adjacency(), G.graph_idx_vec, card, hom, ml)\n",
      "  File \"<ipython-input-3-038ae775592d>\", line 65, in forward\n",
      "    dgm, spl, mask = compute_rips(card, hom_dim, adj, max_length)\n",
      "  File \"<ipython-input-3-038ae775592d>\", line 24, in compute_rips\n",
      "    return compute_persistence(st, card, hom_dim)\n",
      "  File \"<ipython-input-3-038ae775592d>\", line 2, in compute_persistence\n",
      "    st.persistence()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/schraterlab/gebhart/projects/ripsreg/env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/schraterlab/gebhart/projects/ripsreg/env/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/schraterlab/gebhart/projects/ripsreg/env/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/schraterlab/gebhart/projects/ripsreg/env/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/schraterlab/anaconda3/lib/python3.8/inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/schraterlab/anaconda3/lib/python3.8/inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/schraterlab/anaconda3/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/schraterlab/anaconda3/lib/python3.8/inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/schraterlab/anaconda3/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/schraterlab/anaconda3/lib/python3.8/posixpath.py\", line 425, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/schraterlab/anaconda3/lib/python3.8/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "NNG = nn_graph.NNGraph()\n",
    "model = Net()\n",
    "\n",
    "modeldev = Net().to(device)\n",
    "optimizer = torch.optim.Adam(modeldev.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    train(modeldev, model.param_info, NNG, device, train_loader, optimizer, epoch, lbda, percentile, update_every=100)\n",
    "    test(modeldev, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# modeldev = Net().to(device)\n",
    "# optimizer = torch.optim.Adam(modeldev.parameters(), lr=0.001)\n",
    "\n",
    "# for epoch in range(1, 2):\n",
    "#     train_regular(modeldev, device, train_loader, optimizer, epoch)\n",
    "#     test(modeldev, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
